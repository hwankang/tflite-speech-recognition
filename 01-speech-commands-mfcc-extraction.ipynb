{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "01-speech-commands-mfcc-extraction.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-CnuggMEBTW",
        "outputId": "532950e8-d7e0-4039-82a2-33d877e76dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install python_speech_features\n",
        "!pip install librosa\n",
        "from os import listdir\n",
        "from os.path import isdir, join\n",
        "import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import python_speech_features"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5890 sha256=13ccfa8fcc79706d462892ab5f8d2147f31cd55fae0a6dd92574c95622834406\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.48.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (51.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3lh9Q6GEBTb",
        "outputId": "e53ae54b-5d44-44fa-87fd-3a92138b37a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "# Dataset path and view possible targets\n",
        "#/content/sample_data\n",
        "dataset_path ='content/sample_data/speech_commands_dataset'\n",
        "#dataset_path = 'C:\\\\PATH\\\\TO\\\\speech_commands_dataset'\n",
        "for name in listdir(dataset_path):\n",
        "    if isdir(join(dataset_path, name)):\n",
        "        print(name)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d1d7696979c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'content/sample_data/speech_commands_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#dataset_path = 'C:\\\\PATH\\\\TO\\\\speech_commands_dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/sample_data/speech_commands_dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxrUGc67EBTb"
      },
      "source": [
        "# Create an all targets list\n",
        "all_targets = [name for name in listdir(dataset_path) if isdir(join(dataset_path, name))]\n",
        "print(all_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on1NKTAREBTc"
      },
      "source": [
        "# Leave off background noise set\n",
        "all_targets.remove('_background_noise_')\n",
        "print(all_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sldz4jvcEBTc"
      },
      "source": [
        "# See how many files are in each\n",
        "num_samples = 0\n",
        "for target in all_targets:\n",
        "    print(len(listdir(join(dataset_path, target))))\n",
        "    num_samples += len(listdir(join(dataset_path, target)))\n",
        "print('Total samples:', num_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiU0bHjzEBTc"
      },
      "source": [
        "# Settings\n",
        "target_list = all_targets\n",
        "feature_sets_file = 'all_targets_mfcc_sets.npz'\n",
        "perc_keep_samples = 1.0 # 1.0 is keep all samples\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "sample_rate = 8000\n",
        "num_mfcc = 16\n",
        "len_mfcc = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR_0NMy_EBTc"
      },
      "source": [
        "# Create list of filenames along with ground truth vector (y)\n",
        "filenames = []\n",
        "y = []\n",
        "for index, target in enumerate(target_list):\n",
        "    print(join(dataset_path, target))\n",
        "    filenames.append(listdir(join(dataset_path, target)))\n",
        "    y.append(np.ones(len(filenames[index])) * index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ih1yXWtEBTd"
      },
      "source": [
        "# Check ground truth Y vector\n",
        "print(y)\n",
        "for item in y:\n",
        "    print(len(item))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoHtsESBEBTd"
      },
      "source": [
        "# Flatten filename and y vectors\n",
        "filenames = [item for sublist in filenames for item in sublist]\n",
        "y = [item for sublist in y for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iESQhkwEBTd"
      },
      "source": [
        "# Associate filenames with true output and shuffle\n",
        "filenames_y = list(zip(filenames, y))\n",
        "random.shuffle(filenames_y)\n",
        "filenames, y = zip(*filenames_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJsBhCucEBTd"
      },
      "source": [
        "# Only keep the specified number of samples (shorter extraction/training)\n",
        "print(len(filenames))\n",
        "filenames = filenames[:int(len(filenames) * perc_keep_samples)]\n",
        "print(len(filenames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KuYYv2NEBTe"
      },
      "source": [
        "# Calculate validation and test set sizes\n",
        "val_set_size = int(len(filenames) * val_ratio)\n",
        "test_set_size = int(len(filenames) * test_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fGaJXEpEBTe"
      },
      "source": [
        "# Break dataset apart into train, validation, and test sets\n",
        "filenames_val = filenames[:val_set_size]\n",
        "filenames_test = filenames[val_set_size:(val_set_size + test_set_size)]\n",
        "filenames_train = filenames[(val_set_size + test_set_size):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LckdkxlEBTe"
      },
      "source": [
        "# Break y apart into train, validation, and test sets\n",
        "y_orig_val = y[:val_set_size]\n",
        "y_orig_test = y[val_set_size:(val_set_size + test_set_size)]\n",
        "y_orig_train = y[(val_set_size + test_set_size):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXlc0RLmEBTe"
      },
      "source": [
        "# Function: Create MFCC from given path\n",
        "def calc_mfcc(path):\n",
        "    \n",
        "    # Load wavefile\n",
        "    signal, fs = librosa.load(path, sr=sample_rate)\n",
        "    \n",
        "    # Create MFCCs from sound clip\n",
        "    mfccs = python_speech_features.base.mfcc(signal, \n",
        "                                            samplerate=fs,\n",
        "                                            winlen=0.256,\n",
        "                                            winstep=0.050,\n",
        "                                            numcep=num_mfcc,\n",
        "                                            nfilt=26,\n",
        "                                            nfft=2048,\n",
        "                                            preemph=0.0,\n",
        "                                            ceplifter=0,\n",
        "                                            appendEnergy=False,\n",
        "                                            winfunc=np.hanning)\n",
        "    return mfccs.transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYSv-4-qEBTe"
      },
      "source": [
        "# TEST: Construct test set by computing MFCC of each WAV file\n",
        "prob_cnt = 0\n",
        "x_test = []\n",
        "y_test = []\n",
        "for index, filename in enumerate(filenames_train):\n",
        "    \n",
        "    # Stop after 500\n",
        "    if index >= 500:\n",
        "        break\n",
        "    \n",
        "    # Create path from given filename and target item\n",
        "    path = join(dataset_path, target_list[int(y_orig_train[index])], \n",
        "                filename)\n",
        "    \n",
        "    # Create MFCCs\n",
        "    mfccs = calc_mfcc(path)\n",
        "    \n",
        "    if mfccs.shape[1] == len_mfcc:\n",
        "        x_test.append(mfccs)\n",
        "        y_test.append(y_orig_train[index])\n",
        "    else:\n",
        "        print('Dropped:', index, mfccs.shape)\n",
        "        prob_cnt += 1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJqrUf3WEBTf"
      },
      "source": [
        "print('% of problematic samples:', prob_cnt / 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP_pZgLfEBTf"
      },
      "source": [
        "# TEST: Test shorter MFCC\n",
        "# !pip install playsound\n",
        "from playsound import playsound\n",
        "\n",
        "idx = 13\n",
        "\n",
        "# Create path from given filename and target item\n",
        "path = join(dataset_path, target_list[int(y_orig_train[idx])], \n",
        "            filenames_train[idx])\n",
        "\n",
        "# Create MFCCs\n",
        "mfccs = calc_mfcc(path)\n",
        "print(\"MFCCs:\", mfccs)\n",
        "\n",
        "# Plot MFCC\n",
        "fig = plt.figure()\n",
        "plt.imshow(mfccs, cmap='inferno', origin='lower')\n",
        "\n",
        "# TEST: Play problem sounds\n",
        "print(target_list[int(y_orig_train[idx])])\n",
        "playsound(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx-KlBbwEBTf"
      },
      "source": [
        "# Function: Create MFCCs, keeping only ones of desired length\n",
        "def extract_features(in_files, in_y):\n",
        "    prob_cnt = 0\n",
        "    out_x = []\n",
        "    out_y = []\n",
        "        \n",
        "    for index, filename in enumerate(in_files):\n",
        "    \n",
        "        # Create path from given filename and target item\n",
        "        path = join(dataset_path, target_list[int(in_y[index])], \n",
        "                    filename)\n",
        "        \n",
        "        # Check to make sure we're reading a .wav file\n",
        "        if not path.endswith('.wav'):\n",
        "            continue\n",
        "\n",
        "        # Create MFCCs\n",
        "        mfccs = calc_mfcc(path)\n",
        "\n",
        "        # Only keep MFCCs with given length\n",
        "        if mfccs.shape[1] == len_mfcc:\n",
        "            out_x.append(mfccs)\n",
        "            out_y.append(in_y[index])\n",
        "        else:\n",
        "            print('Dropped:', index, mfccs.shape)\n",
        "            prob_cnt += 1\n",
        "            \n",
        "    return out_x, out_y, prob_cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHYcZ1FEBTf"
      },
      "source": [
        "# Create train, validation, and test sets\n",
        "x_train, y_train, prob = extract_features(filenames_train, \n",
        "                                          y_orig_train)\n",
        "print('Removed percentage:', prob / len(y_orig_train))\n",
        "x_val, y_val, prob = extract_features(filenames_val, y_orig_val)\n",
        "print('Removed percentage:', prob / len(y_orig_val))\n",
        "x_test, y_test, prob = extract_features(filenames_test, y_orig_test)\n",
        "print('Removed percentage:', prob / len(y_orig_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy-J1osVEBTg"
      },
      "source": [
        "# Save features and truth vector (y) sets to disk\n",
        "np.savez(feature_sets_file, \n",
        "         x_train=x_train, \n",
        "         y_train=y_train, \n",
        "         x_val=x_val, \n",
        "         y_val=y_val, \n",
        "         x_test=x_test, \n",
        "         y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6YrRe0rEBTg"
      },
      "source": [
        "# TEST: Load features\n",
        "feature_sets = np.load(feature_sets_file)\n",
        "feature_sets.files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bArHPJ2xEBTg"
      },
      "source": [
        "len(feature_sets['x_train'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_ZmfhuMEBTg"
      },
      "source": [
        "print(feature_sets['y_val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmZQUgpGEBTh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}